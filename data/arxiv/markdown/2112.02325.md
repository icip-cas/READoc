# A Russian Jeopardy! Data Set for Question-Answering Systems

Elena Mikhalkova

0000-0003-0781-8633

###### Abstract

Question answering (QA) is one of the most common NLP tasks that relates to named entity recognition, fact extraction, semantic search and some other fields. In industry, it is much appreciated in chat-bots and corporate information systems. It is also a challenging task that attracted the attention of a very general audience at the quiz show Jeopardy! In this article we describe a Jeopardy!-like Russian QA data set collected from the official Russian quiz database Chgk _che-ge-‘ka:_. The data set includes 379,284 quiz-like questions with 29,375 from the Russian analogue of Jeopardy! – “Own Game”. We observe its linguistic features and the related QA-task. We conclude about perspectives of a QA competition based on the data set collected from this database.

Keywords: Question answering Open-domain Quiz Jeopardy! Own game Chgk Linguistic data set Competition Evaluation

## 1 Introduction

In natural language processing (NLP), question answering (QA) is one of the most common tasks that encompasses a number of question types, including “questions about everything”, the so-called open-domain QA [3]. The latter cover a wide range of topics and do not necessarily come in form of an actual question (e.g. “Who is the living Queen of England?”) which draws the task of answering them very close to information retrieval. Instead it can be just a line of keywords: _living Queen England_. From this broad perspective, QA flourishes in production of search engines, corporate information systems and conversational technologies like chat-bots.

In February 2011, Watson, an IBM’s information system [6] installed in a small computer, won against two very prominent human players in a TV quiz-show called Jeopardy! ¹ The algorithm was trained on TREC corpus [12] and 500 questions manually collected from the TV-show. In TREC, questions are formulated quite typically, e.g. “How many calories are there in a Big Mac?”, although they cover a variety of topics. In contrast to it, the Jeopardy! challenge presents questions as clues narrowed by a certain domain like in the following example from [6]:

Footnote 1: https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html

_Category:_ Oooh….Chess

_Clue:_ Invented in the 1500s to speed up the game, this maneuver involves two pieces of the same color.

_Answer:_ Castling

The Russian QA data sets are more like trivia questions and answers resembling TREC: RuBQ [7] consists of 1,500 Russian questions loaded from various “quiz collections on the Web” with answers linked to Wikidata entities; RuBQ 2.0 has “2,910 questions along with the answers and SPARQL queries” [11]; SberQuAD [5] contains “50,364 paragraph–question–answer triples” that are now publicly available; the questions were written by crowd-workers.

In this article, we observe a data set of Russian Jeopardy! questions and answers and outline a related QA task. The data base of questions and answers called Chgk _che-ge-‘ka:_ is freely available at https://db.chgk.info/. Our current contribution includes the following:

1. 1.We describe the Russian Chgk QA database.
2. 2.We describe a corpus of Jeopardy! questions collected from the Chgk database.
3. 3.We formulate the QA-task based on the corpus.

## 2 Russian Professional Quiz Database

There exists a variety of Russian intellectual games (quizzes) some of which have formed very devoted communities not only in Russia. “What? Where? When?” (_Chto? Gde? Kogda?_, hence the abbreviation Ch-G-K) is, probably, the most popular Russian TV quiz show dating back to 1975 ². As the TV-game only allows few players (a team of six per one episode), in the 1990s the game spread among common people who wrote questions and played them at local tournaments. The movement grew into the so-called “Sport Chgk”. Presently, its tournaments are organized in Montreal, Richmond Hill, Vilnius, Odessa, Cologne, Boston, Nahariya, Eilat, Parnu, Astana, Vladivostok and many other cities. The movement has an official database which is available via the mentioned resource. The earliest tournament in the database is the 1990’s “I Championship MAK in “What? Where? When?” 1990-01-01”. ³ The copyright allows to use its questions for non-commercial purposes with some of the packs (collections of questions played during one tournament) distributed under different Creative Commons licenses ⁴. Packs are written by professional authors, tested and approved by other authors and then played at different events (offline and online) under different commercial terms. After a row of tournaments, packs are uploaded to the database. Amateur and semi-professional packs usually do not go to the official database. The moment packs are uploaded, they are under the database copyright. As the site allows only specific search in the database, we parsed the XML-tree of tournaments at https://db.chgk.info/tour with the Python library BeautifulSoup ⁵ and gathered all the QA information from HTML-pages. The general metadata include: Question, Answer, Author, Sources (Web-links that authors used to write a question), Comments (by authors and organizers), Pass Criteria (in case players’ answers are not very precise), Notices (comments by players), Images (Web-links to pictures if they are needed in a question), Rating (hardness of the question calculated from how many teams failed to answer it), Number of question (in each tournament), Tour (tour=tournament) type. The metadata for Jeopardy! questions also include Topic (a common topic for a set of 5 questions) and Topic Number (in the order of sets of questions from one tournament). The data were collected in form of .csv tables locally and uploaded to our own SQL-database (see a part of its scheme referring to the corpus in Fig. 1).

Footnote 2: https://en.wikipedia.org/wiki/What%3F_Where%3F_When%3F

Footnote 3: https://db.chgk.info/tour/mak1

Footnote 4: https://db.chgk.info/copyright

Footnote 5: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#

Figure 1: A part of our loaded Chgk database illustrating metadata about questions.

As mentioned, the sport Chgk includes questions of different types depending on the tournament that they are played at. They can even be in a poetic form, but most of them are typical Chgk questions. Let us illustrate them with an example.

_Question 7, the tournament “ChGK is… - 2017”:_⁶ The legend has it that once Paul Bunyan fired his gun at a deer and ran to get his prey. But he ran so fast that he DID THIS and felt an itch in his back. What did he do?

Footnote 6: Authors V. Ostrovskiy, A. Boyko, M. Podryadchikova. Translation into English is ours. https://db.chgk.info/tour/eila08al.2

_Answer:_ He outran the bullet.

There are formulas for writing a Chgk question. The example above has a very common formula “DID THIS”. Although it mentions the detail – the American folk hero Paul Bunyan and a deer, the answer does not require this information. It can be derived only from the situation with the bullet, running and an itch in the back. This is a way of misleading players that grab at several hints and need to choose the correct semantic, logical and factual track that narrows the choice of answer. It is important that answers derived from wrong tracks should be incoherent or contradict some facts that are omitted in the question, so that the correct answer cannot be criticized.

The Chgk database also contains questions of Jeopardy! type ⁷. Let us study the following example.

Footnote 7: In Russian, Jeopardy! is called “Own Game” _Svoya Igra_. https://www.imdb.com/title/tt1381017/

_Topic 7: Parrots. Author Oleg Sarayev._⁸ The last name of THIS famous DETECTIVE is translated as “a parrot”.

Footnote 8: Translation into English is ours. https://db.chgk.info/tour/eu05stsv

_Answer:_ Hercule Poirot.

Note that this question is shorter and more fact-oriented. It requires to compare two rows of data: words denoting “parrot” in different languages and last names of famous detectives. “Own Game” is usually played by single players, although in sport Chgk there are variants for teams of two, three and four. However, some Chgk questions are also very close to “Own Game”, especially in tournaments called _lite_, i.e. from easier tournaments for new-comers and younger players.

## 3 Russian Jeopardy! Data Set

As mentioned, we downloaded data about questions from the official sport Chgk resource to be able to parse them and store in different formats. The Russian Jeopardy! (_Own Game_) data set seems to us to be the most valuable for NLP as:

1. 1.its questions are shorter than in other quizzes and more fact-oriented;
2. 2.it is quality-guaranteed, as it was created by professional authors;
3. 3.it is suitable for open-domain QA;
4. 4.it has additional information like links to Web-sources and question ranking that points at its “hardness”;
5. 5.above all, it is not too trivial in the field of QA data sets and hence it can foster new tasks and approaches in QA itself.

The last point is more vividly discussed by [1].

Table 1 gives a summary of the Russian Jeopardy! data set, as downloaded on 6 July 2021 (and recently updated on 18 November 2021). In the table, “Synchron” is a typical Chgk tournament played immediately by several teams of six players maximum; “Lite” is its mentioned version with easier questions. It has shorter questions, but unlike Jeopardy! it is more logical. The table shows that Jeopardy! questions are twice shorter in length than typical Chgk questions. And even lite questions are not near them in length.

\begin{table}
\begin{tabular}{|l|l|l|l|l|}
\hline
Type & Questions & Tours & Average Q length & Average Q length \\
 & & & in tokens & in symbols \\
\hline
Jeopardy! & 29,375 & 452 & 14.28 & 98.37 \\
Chgk Synchron & 48,065 & 1,821 & 32 & 234 \\
Chgk Lite & 1,936 & 54 & 27.5 & 201 \\
\hline
All & 379,284 & 4,816 & 34 & 244.9 \\ \hline
\end{tabular}
\end{table}
Table 1: Details about the sport Chgk database, as of 1 July 2021. Tours – tournaments; Q – question.

## 4 Task Discussion and Concluding Remarks

The QA task corresponding to the data set is trivial: the system reads a question and returns an answer. Then an evaluation metric, first, compares the answer to the correct one and, second, calculates the system’s performance. Thus, the evaluation stage consists of two steps, the second of which has been already widely discussed, see for example [4, 2]. However, the first step is more varied across projects as correctness of the answer is a scalable value. In case of the answer to the previously discussed question, the following variants should be considered correct: “Hercule Poirot”, “It is Hercule Poirot.”, “Poirot”, etc. Hence, the correctness can be evaluated as the minimum limit calculated by the following metrics: Levenstein distance [8], cosine similarity based on semantic document vectors of the system’s answer and the correct answer [10], Jaccard Coefficient for keyword similarity [9] (e.g. keywords _Hercule_ and _Poirot_). Also, of course, there can be a manual check for borderline cases.

Our training data set for a Russian QA Jeopardy! challenge contains 29,375 questions from the Chgk database. The questions were selected based on the following criteria:

1. 1.a question is in form of a text;
2. 2.a question does not have an image supporting it;
3. 3.a question does not mention that any images should be distributed or shown on a screen while solving it.

I.e. these are fully verbalized questions. The data set including some “flattened” metadata from the database scheme 1 is placed in .csv at https://github.com/evrog/Russian-QA-Jeopardy. The delimiter is tabulation. The data include: Question ID, Question, Answer, Topic, Authors’ Full Names, Name of tournament, Link to Tournament. Comments and Sources have not been included in the data set as they will not be in the test set.

As for the test set, due to the fact that the data set of Russian Jeopardy! is open and its questions are easily accessed via search machines and the database interface, there is a need for evaluation data (a closed test set). We have prepared a pack of yet unpublished questions written by authors of sport Chgk that will be distributed at the evaluation stage.

Summing up, we have introduced a data set of Russian Jeopardy! questions and answers. We have described its peculiarities that make it possible to use it in a QA competition. We have also described approaches to automatic evaluation of correctness of answers. Finally, we plan a QA challenge based on this data set including an unpublished set of questions.

## References

* [1] Boyd-Graber, J., Börschinger, B.: What question answering can learn from trivia nerds. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7422–7435. Association for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.662, https://aclanthology.org/2020.acl-main.662
* [2] Calijorne Soares, M.A., Parreiras, F.S.: A literature review on question answering techniques, paradigms and systems. Journal of King Saud University - Computer and Information Sciences **32**(6), 635–646 (2020). https://doi.org/https://doi.org/10.1016/j.jksuci.2018.08.005, https://www.sciencedirect.com/science/article/pii/S131915781830082X
* [3] Chen, D., Yih, W.t.: Open-domain question answering. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts. pp. 34–37 (2020)
* [4] Dimitrakis, E., Sgontzos, K., Tzitzikas, Y.: A survey on question answering systems over linked data and documents. Journal of Intelligent Information Systems **55**(2), 233–259 (2020)
* [5] Efimov, P., Chertok, A., Boytsov, L., Braslavski, P.: Sberquad–russian reading comprehension dataset: Description and analysis. In: International Conference of the Cross-Language Evaluation Forum for European Languages. pp. 3–15. Springer (2020)
* [6] Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A.A., Lally, A., Murdock, J.W., Nyberg, E., Prager, J., et al.: Building watson: An overview of the deepqa project. AI magazine **31**(3), 59–79 (2010)
* [7] Korablinov, V., Braslavski, P.: Rubq: a russian dataset for question answering over wikidata. In: International Semantic Web Conference. pp. 97–110. Springer (2020)
* [8] Levenshtein, V.I., et al.: Binary codes capable of correcting deletions, insertions, and reversals. In: Soviet physics doklady. vol. 10:8, pp. 707–710. Soviet Union (1966)
* [9] Niwattanakul, S., Singthongchai, J., Naenudorn, E., Wanapu, S.: Using of jaccard coefficient for keywords similarity. In: Proceedings of the international multiconference of engineers and computer scientists. vol. 1:6, pp. 380–384 (2013)
* [10] Rahutomo, F., Kitasuka, T., Aritsugi, M.: Semantic cosine similarity. In: The 7th International Student Conference on Advanced Science and Technology ICAST. vol. 4:1, p. 1 (2012)
* [11] Rybin, I., Korablinov, V., Efimov, P., Braslavski, P.: Rubq 2.0: An innovated russian question answering dataset. In: European Semantic Web Conference. pp. 532–547. Springer (2021)
* [12] Voorhees, E.: The trec-8 question answering track report. In: NIST Special Publication 500-246: The Eighth Text REtrieval Conference (TREC 8) (1999)

