# SEGSRNet for Stereo-Endoscopic Image Super-Resolution and Surgical Instrument Segmentation

Mansoor Hayat

_Dept. of Electrical Engineering_
_Chulalongkorn University_
Bangkok, Thailand
6471015721@student.chula.ac.th

Supavadee Aramvith

_Dept. of Electrical Engineering_
_Chulalongkorn University_
Bangkok, Thailand
supavadee.a@chula.ac.th

Titipat Achakulvisut

_Dept. of Biomedical Engineering_
_Mahidol University_
Bangkok, Thailand
titipat.ach@mahidol.edu

###### Abstract

SEGSRNet addresses the challenge of precisely identifying surgical instruments in low-resolution stereo endoscopic images, a common issue in medical imaging and robotic surgery. Our innovative framework enhances image clarity and segmentation accuracy by applying state-of-the-art super-resolution techniques before segmentation. This ensures higher-quality inputs for more precise segmentation. SEGSRNet combines advanced feature extraction and attention mechanisms with spatial processing to sharpen image details, which is significant for accurate tool identification in medical images. Our proposed model, SEGSRNet, surpasses existing models in evaluation metrics including PSNR and SSIM for super-resolution tasks, as well as IoU and Dice Score for segmentation. SEGSRNet can provide image resolution and precise segmentation which can significantly enhance surgical accuracy and patient care outcomes.

Index Terms: robotic surgery, segmentation, stereo endoscopic surgical imaging, super-resolution, surgical instruments

## I Introduction

The advancement of digital imaging technology, from early monochromatic photography to modern 8k resolution, plays a pivotal role in various fields, including medical diagnostics, where image clarity is essential [1]. In medical imaging, high-resolution techniques are crucial, particularly in diagnostics and surgical procedures, underscoring the importance of super-resolution (SR) techniques to overcome issues like lens limitations [2][16].

In stereo image SR, maintaining view consistency is vital, with recent developments like the Parallax Attention Module in Disparity Constraint Stereo SR (DCSSR) [2] and bi Directional Parallax Attention Map (biPAM) in iPASSR [5] enhancing perceptual quality. Accurate identification and segmentation of surgical instruments in images are important, for which advanced semantic segmentation techniques are employed, leveraging CNNs and architectures like U-Net [3] for improved accuracy.

Our research integrates SR and segmentation technologies for robotic-assisted surgeries. We introduce a hybrid model that applies SR before segmentation, enhancing the accuracy with high-quality inputs. This model, benchmarked against established methods like UNet [3] and TernausNet [4], shows superior performance in both SR and segmentation domains, demonstrating its efficacy in complex medical imaging tasks.

Figure 1: Proposed SEGSRNet Architecture. A. SEGSRNet architecture consists of super-resolution and segmentation modules. B. Proposed cross-view attention module and residual dense block in super-resolution framework.

Figure 2: SPP-LinkNet34 structure highlighting the encoder-decoder network with spatial pyramid pooling for enhanced multi-scale feature extraction in image segmentation tasks.

## II Research Methodology

Advancements in super-resolution (SR) techniques, especially the biPAM network, have significantly improved surgeons’ sensory capabilities in medical settings. Demonstrating its effectiveness in the NTIRE 2022 Challenge [6], biPAM excels in learning cross-view information, which is pivotal for high-quality SR stereo images. This process involves downscaling high-resolution (HR) images to create low-resolution (LR) counterparts, which are then enhanced through a feature extraction module comprising a combined channel and spatial attention (CCSA) and an Atrous Spatial Pyramid Pooling (ASPP) block, followed by Residual Dense Blocks (RDB). The network culminates in SR image reconstruction, leveraging multi-attention biPAM.

Figure 3: Assessment of the Visual Quality of High-Resolution Images Created Through Image Super-Resolution Techniques at a \(\times 4\) Scale Factor.

Our proposed network addresses the challenge of semantic segmentation in surgery by distinguishing surgical instruments from the background in super-resolution images, enhancing both medical imaging and robotic surgery.

### _Super-Resolution Part_

#### II-A1 Feature Extraction and Refinement Blocks

Our model features a Combined Channel and Spatial Attention Block (CCSB)[7], which includes a Channel Attention Block (CAB) for enhancing feature maps and a Spatial Channel Attention Block (SAB) for focusing on key regions. The features processed through CCSB are further refined using an Atrous Spatial Pyramid Pooling (ASPP) block and Residual Dense Blocks (RDBs). These components deepen feature extraction and create a comprehensive feature hierarchy, significantly improving the model’s performance in super-resolution.

#### II-A2 Cross-View Feature Interaction Module

Integrating multi-scale attention into biPAM enhances the interaction and integration of cross-view information in stereo features, which is key for precise stereo correspondence. This improvement is achieved through hierarchical feature representation by combining output features from each Residual Dense Block (RDB) in the feature extraction module [15].

Inputs to biPAM undergo processing through batch normalization and a transition residual block (ResB), followed by 1×1 convolutions, producing feature tensors \(FU\) and \(FV\):

\[F_{X}(h,w,c)=F_{X}(h,w,c)-\frac{1}{W}\sum_{i=1}^{W}F_{X}(h,i,c)\quad\text{for }X\in{U,V}\] (1)

The multi-scale attention mechanism enhances stereo-image processing by adaptively focusing on and integrating details from different resolution levels, cardinal for accurately reconstructing depth information. Attention maps \(M_{R\to L}\) and \(M_{L\to R}\) facilitate cross-view interaction:

\[F_{X\to Y}=M_{X\to Y}\otimes F_{X}\quad\text{for }(X,Y)\in\{(R,L)\}.\] (2)

The occlusion handling scheme computes valid masks \(V_{L}\) and \(V_{R}\), ensuring continuous spatial distributions by filling occluded regions with features from the target view:

\[F_{X\to Y}=V_{Y}\cdot F_{Y\to X}+(1-V_{Y})\cdot F_{Y}\quad\text{for }(X,Y)\in\{(R,L)\}.\] (3)

This module significantly enhances stereo image processing by effectively improving feature interaction and managing occlusions.

#### II-A3 Reconstruction Block

In our model’s reconstruction block, a refinement block combines \(F_{R\to L}\) with \(F_{L}\), followed by processing through a Residual Dense Block (RDB) and a channel attention layer (CALayer). This sequence, including additional RDBs, convolution layers, and a sub-pixel layer, significantly enhances feature fusion and image quality, leading to a high-precision, super-resolved image.

### _Segmentation Part_

Binary segmentation differentiates between foreground and background, parts segmentation identifies individual components of objects, and type segmentation classifies each pixel based on object categories, enhancing scene comprehension and object interaction analysis. SPP-LinkNet-34, depicted in Fig. 2, features an architecture optimized for effective segmentation with an encoder-decoder structure. It employs convolution techniques, batch normalization, and ReLU non-linearity [9], [10]. The encoder utilizes a 7×7 kernel and spatial max-pooling, followed by residual blocks [11], while the decoder is designed for efficient feature mapping.

A notable aspect of SPP-LinkNet-34 is its use of the lighter ResNet18 as its encoder [8], and the inclusion of a Spatial Pyramid Pooling (SPP) block that enhances multi-scale input handling. This design allows SPP-LinkNet-34 to recover spatial information lost during downsampling efficiently, resulting in improved segmentation accuracy and efficiency, suitable for real-time applications.

### _Datasets_

We use The MICCAI 2018 Robotic Scene Segmentation Sub-Challenge [12] (”MICCAI 2018”) and the MICCAI 2017 Robotic Instrument Segmentation Challenge [13] (”EndoVis 2017”) are the datasets for the robotic scene segmentation in endoscopic procedures. Both challenges provide high-resolution stereo image pairs \(1280\times 1024\) pixels) captured during porcine training sessions, along with camera calibrations. For the SR task, we trained our models using the MICCAI 2018 dataset and then evaluated their performance using both the MICCAI 2018 and EndoVis 2017 datasets. Conversely, for segmentation, we conducted both training and testing exclusively on the EndoVis 2017 dataset. We calculate Peak signal-to-noise ratio (PSNR) and Structural Similarity (SSIM) as evaluation metrics. For segmentation performace, we use 10-folds cross-validation to measure segmentation Intersection over Union (IoU) on EndoVis 2017 and calculate the mean and standard deviation (STD) of the validation set.

### _Training Settings_

Our proposed model was implemented using the Pytorch 2.0 framework and trained on a Nvidia 3090Ti GPU. We employed Xavier initialization for the model’s parameters and used Adam optimizer with an initial learning rate of \(3\times 10^{-4}\). All models are trained for 100 epochs. We used a batch size of 6 for images at scale two (\(\times 2\)) and a batch size of 5 for images at scale four (\(\times 4\)).

## III Experimental Results

Figure 4: Comparative Evaluation of Segmentation Performance: Our Model versus Current State-of-the-Art Models.

\begin{table}
\begin{tabular}{|p{}|p{}|p{}|p{}|}
\hline
**Method** & **Scale** & **MICCAI 2018 (PSNR/SSIM) \(\uparrow\)** & **EndoVis 2017 (PSNR/SSIM) \(\uparrow\)** \\
\hline
bicubic & \(\times 2\) & 38.60/0.9792 & 27.07/0.9594 \\
\hline
SRCNN & \(\times 2\) & 38.99/0.9811 & 28.89/0.9646 \\
\hline
VDSR & \(\times 2\) & 39.57/0.9824 & 29.23/0.9654 \\
\hline
DRRN & \(\times 2\) & 40.18/0.9858 & 32.19/0.9666 \\
\hline
StereoSR & \(\times 2\) & 40.25/0.9859 & 36.18/0.9912 \\
\hline
PASSRNet & \(\times 2\) & 40.36/0.9860 & 40.36/0.9921 \\
\hline
iPASSRNet & \(\times 2\) & 41.01/0.9866 & 40.57/0.9941 \\
\hline
DCSSRNet & \(\times 2\) & 41.09/0.9866 & 40.03/0.9917 \\
\hline
CCSBESR & \(\times 2\) & 41.99/0.9871 & 40.38/0.9920 \\
\hline
SEGSRNet (Our) & \(\times 2\) & **42.41/0.9879** & **41.87/0.9965** \\
\hline \hline
**Method** & **Scale** & **MICCAI 2018 (PSNR/SSIM) \(\uparrow\)** & **EndoVis 2017 (PSNR/SSIM) \(\uparrow\)** \\
\hline
bicubic & \(\times 4\) & 32.85/0.9480 & 25.72/0.9436 \\
\hline
SRCNN & \(\times 4\) & 33.11/0.9510 & 26.71/0.9513 \\
\hline
VDSR & \(\times 4\) & 33.35/0.9516 & 27.06/0.9518 \\
\hline
DRRN & \(\times 4\) & 34.01/0.9558 & 28.79/0.9624 \\
\hline
StereoSR & \(\times 4\) & 34.08/0.9545 & 34.04/0.9669 \\
\hline
PASSRNet & \(\times 4\) & 34.12/0.9547 & 36.83/0.9699 \\
\hline
iPASSRNet & \(\times 4\) & 34.52/0.9549 & 37.76/0.9710 \\
\hline
DCSSRNet & \(\times 4\) & 34.76/0.9553 & 33.52/0.9719 \\
\hline
CCSBESR & \(\times 4\) & 34.99/0.9558 & 37.91/0.9725 \\
\hline
SEGSRNet (Our) & \(\times 4\) & **36.01/0.9768** & **38.33/0.9924** \\ \hline
\end{tabular}
\end{table}
TABLE I: **Performance Evaluation of Enlargement Factors \(\times 2\) And \(\times 4\) On MICCAI 2018 And EndoVis 2017: A Quantitative Analysis Using PSNR/SSIM.**

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{**Methods**} & \multicolumn{2}{c|}{**Binary segmentation**} & \multicolumn{2}{c|}{**Parts segmentation**} & \multicolumn{2}{c|}{**Type segmentation**} \\
\cline{2-7}
 & **IoU(\%) \(\uparrow\)** & **Dice(\%) \(\uparrow\)** & **IoU(\%) \(\uparrow\)** & **Dice(\%) \(\uparrow\)** & **IoU(\%) \(\uparrow\)** & **Dice(\%) \(\uparrow\)** \\
\hline
U-Net & 75.44 ± 18.18 & 84.37 ± 14.58 & 48.41 ± 17.59 & 60.75 ± 18.21 & 15.80 ± 15.06 & 23.59 ± 19.87 \\
\hline
TernausNet & 81.14 ± 19.11 & 88.07 ± 14.63 & 62.23 ± 16.48 & 74.25 ± 15.55 & 34.61 ± 20.53 & 45.86 ± 23.20 \\
\hline
LinkNet-34 & 82.36 ± 18.77 & 88.87 ± 14.35 & 34.55 ± 20.96 & 41.26 ± 23.44 & 22.47 ± 35.73 & 24.71 ± 37.54 \\
\hline
PlainNet & 81.86 ± 15.85 & 88.96 ± 12.98 & 64.73 ± 17.39 & 73.53 ± 16.98 & 34.57 ± 21.93 & 44.64 ± 25.16 \\
\hline
Nested UNet & 82.94 ± 16.82 & 89.42 ± 14.01 & 58.38 ± 19.06 & 69.59 ± 18.66 & **41.72 ± 33.44** & **48.22 ± 34.46** \\
\hline
**SPP-LinkNet34 (Our)** & **83.65 ± 16.47** & **89.80 ± 13.99** & **66.87 ± 17.10** & **76.93 ± 16.08** & 15.96 ± 13.78 & 23.79 ± 18.88 \\ \hline
\end{tabular}
\end{table}
TABLE II: Analysis of Segmentation Performance for Instruments Across Three Different Tasks (Mean ± Standard Deviation)

### _Quantitative and Qualitative Results_

Our model outperforms traditional U-Net by 9.81% and 27.60% in binary and parts segmentation in terms of IoU. However, our model had limitations in type segmentation due to its emphasis on global contextual information which is less suited for the fine-grained, pixel-level distinctions required among multiple complex classes, compared to the more generalized tasks in binary and parts segmentation.

The super-resolution results show that SEGSRNet corrects inaccuracies and computes disparities more effectively than traditional methods such as Bicubic interpolation and DRRN (Fig 3). Overall, SEGSRNet outperforms traditional model in both \(\times 2\) and \(\times 4\). After applying SR, we illustrates the model’s proficiency in various segmentation tasks, including binary, Parts, and Type segmentation, highlighting its superior performance in accurately segmenting different image components (Fig 4).

## IV Conclusion

SRSEGNet introduces a breakthrough in deep learning for super-resolution and segmentation in endoscopic vision, leveraging convolutional neural networks and SPP-LinkNet-34. Achieving high performance on the EndoVis 2017 dataset, SRSEGNet excels in binary segmentation with an IoU of 83.65% and a Dice score of 89.80%, effectively handling complex multi-class segmentation tasks.

## References

* [1] Higgins, Rana M and Frelich, Matthew J and Bosler, Matthew E and Gould, Jon C,_Cost analysis of robotic versus laparoscopic general surgery procedures_,_Surgical endoscopy_, vol. 31, pp. 185-192, 2017.
* [2] Zhang, Tianyi and Gu, Yun and Huang, Xiaolin and Yang, Jie and Yang, Guang-Zhong, Disparity-constrained stereo endoscopic image super-resolution, International Journal of Computer Assisted Radiology and Surgery, vol. 17, pp. 867-875, 2022.
* [3] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in Proc. Int. Conf. Med. Imag Comput. Comput. Assist. Interv. Springer, 2015, pp. 234–241.
* [4] V. Iglovikov and A. Shvets, “Ternausnet: U-net with vgg11 encoder pre-trained on imagenet for image segmentation,” arXiv preprint arXiv:1801.05746, 2018.
* [5] Wang, Yingqian and Ying, Xinyi and Wang, Longguang and Yang, Jungang and An, Wei and Guo, Yulan ,Symmetric parallax attention for stereo image super-resolution,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 766-775.
* [6] Wang, Longguang and Guo, Yulan and Wang, Yingqian and Li, Juncheng and Gu, Shuhang and Timofte, Radu and Chen, Liangyu and Chu, Xiaojie and Yu, Wenqing and Jin, Kai, NTIRE 2022 challenge on stereo image super-resolution: Methods and results, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 906-919.
* [7] M. Hayat, S. Armvith and T. Achakulvisut, 2023. Combined Channel and Spatial Attention-Based Stereo Endoscopic Image Super Resolution, _TENCON 2023 - 2023 IEEE Region 10 Conference (TENCON), Chiang Mai, Thailand_, pp. 920-925, doi: 10.1109/TENCON58879.2023.10322331.
* [8] J.Long,E.Shelhamer, andT.Darrell, “Fully convolutional networks for semantic segmentation,” Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition,2015 , pp.3431–3440
* [9] V.Nair andG. E.Hinton, “Rectified linear units improve restricted boltzmann machines,”in Proceedings of the 27th international conference on machine learning(ICML-10),2010,pp.807–814.
* [10] S. Ioffe andC. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariateshift,” arXivpreprint arXiv:1502.03167,2015.
* [11]K.He,X.Zhang,S.Ren,andJ.Sun,“Deep residual learning for image recognition,”arXivpreprintarXiv:1512.03385,2015.
* [12] Allan, Max and Kondo, Satoshi and Bodenstedt, Sebastian and Leger, Stefan and Kadkhodamohammadi, Rahim and Luengo, Imanol and Fuentes, Felix and Flouty, Evangello and Mohammed, Ahmed and Pedersen, Marius , _2018 robotic scene segmentation challenge_, _arXiv preprint arXiv:2001.11190_,2020.
* [13] Allan, Max, Alex Shvets, Thomas Kurmann, Zichen Zhang, Rahul Duggal, Yun-Hsuan Su, Nicola Rieke et al. ”2017 robotic instrument segmentation challenge.” arXiv preprint arXiv:1902.06426 (2019).
* [14] Wang, Longguang and Wang, Yingqian and Liang, Zhengfa and Lin, Zaiping and Yang, Jungang and An, Wei and Guo, Yulan,Learning parallax attention for stereo image super-resolution,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12250-12259.
* [15]Wang, Longguang, Yulan Guo, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, and Wei An. ”Parallax attention for unsupervised stereo correspondence learning.” IEEE transactions on pattern analysis and machine intelligence 44, no. 4 (2020): 2108-2125.
* [16] Hayat, Mansoor, and Supavadee Aramvith. ”E-SEVSR-Edge Guided Stereo Endoscopic Video Super-Resolution.” IEEE Access (2024).

